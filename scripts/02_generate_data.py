"""
ä¿®å¤ç‰ˆè®­ç»ƒæ•°æ®ç”Ÿæˆå™¨
æ ¸å¿ƒæ”¹è¿›:
1. ç›´æ¥åŸºäºä»£ç å†…å®¹ç”Ÿæˆå‡†ç¡®çš„é—®ç­”å¯¹
2. ä¸ä¾èµ–LLMç”Ÿæˆï¼ˆé¿å…å¾ªç¯ä¾èµ–ï¼‰
3. ä½¿ç”¨æ¨¡æ¿åŒ–æ–¹æ³•ç¡®ä¿æ•°æ®è´¨é‡
4. ä¼˜åŒ–é¡¹ç›®æ¦‚è§ˆé—®é¢˜ï¼Œä½¿å…¶æ›´å…·é¡¹ç›®ç‰¹è‰²
"""
import json
import yaml
import random
from pathlib import Path
from typing import List, Dict, Any
from dataclasses import dataclass, field # <--- ä¿®å¤: dataclass ä½äº dataclasses æ¨¡å—
import re
from collections import defaultdict


@dataclass
class TrainingSample:
    """è®­ç»ƒæ ·æœ¬"""
    conversations: List[Dict[str, str]]
    metadata: Dict[str, Any]


class FixedDataGenerator:
    """ä¿®å¤ç‰ˆæ•°æ®ç”Ÿæˆå™¨ - åŸºäºè§„åˆ™å’Œæ¨¡æ¿"""
    
    def __init__(self, config_path: str = "../config/default_config.yaml", 
                 analysis_path: str = "../data/repository_analysis.json"):
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
        
        try:
            with open(analysis_path, 'r', encoding='utf-8') as f:
                self.analysis_data = json.load(f)
        except FileNotFoundError:
            print(f"âŒ è­¦å‘Š: æ‰¾ä¸åˆ°åˆ†ææ–‡ä»¶ {analysis_path}ã€‚è¯·å…ˆè¿è¡Œåˆ†æå™¨ã€‚")
            self.analysis_data = {'code_elements': [], 'project_context': {}}

        self.code_elements = self.analysis_data.get('code_elements', [])
        self.project_context = self.analysis_data.get('project_context', {})
        self.project_name = self.project_context.get('project_name', 'Laddr')
        
        self.training_samples = []
    
    def generate_training_data(self):
        """ç”Ÿæˆè®­ç»ƒæ•°æ®"""
        print(f"Generating training data for {self.project_name}...")
        
        # 1. ä»£ç è§£é‡Šä»»åŠ¡ï¼ˆåŸºäºdocstring + ä»£ç ç»“æ„ï¼‰
        print("Generating code explanation samples...")
        self._generate_code_explanation_samples()
        
        # 2. APIä½¿ç”¨ç¤ºä¾‹ï¼ˆåŸºäºå‡½æ•°ç­¾å + docstringï¼‰
        print("Generating API usage samples...")
        self._generate_api_usage_samples()
        
        # 3. é¡¹ç›®æ¦‚è§ˆé—®ç­”ï¼ˆåŸºäºç»Ÿè®¡å’Œç»“æ„ä¿¡æ¯ï¼‰
        print("Generating project overview samples...")
        self._generate_project_overview_samples()
        
        # 4. ä»£ç å®šä½ä»»åŠ¡ï¼ˆ"åœ¨å“ªä¸ªæ–‡ä»¶ä¸­..."ï¼‰
        print("Generating code location samples...")
        self._generate_code_location_samples()
        
        print(f"Total samples generated: {len(self.training_samples)}")
    
    def _generate_code_explanation_samples(self):
        """ç”Ÿæˆä»£ç è§£é‡Šæ ·æœ¬ - åŸºäºçœŸå®ä»£ç å’Œdocstring"""
        # é€‰æ‹©æœ‰docstringçš„å…ƒç´ 
        candidates = [e for e in self.code_elements 
                     if e.get('docstring') and len(e.get('code', '')) > 50]
        
        for element in candidates[:300]:  # å¢åŠ æ•°é‡é™åˆ¶
            name = element['name']
            docstring = element['docstring']
            filepath = element['filepath']
            element_type = element['type']
            code = element.get('code', '')
            
            # æå–å‡½æ•°ç­¾å
            signature = self._extract_signature(code, element_type)
            
            # é—®é¢˜æ¨¡æ¿
            questions = [
                f"è¯·è§£é‡Š {self.project_name} ä¸­ `{name}` çš„ä½œç”¨ã€‚",
                f"{self.project_name} çš„ `{name}` æ˜¯åšä»€ä¹ˆçš„ï¼Ÿ",
                f"åœ¨ {self.project_name} é¡¹ç›®ä¸­ï¼Œ`{name}` æœ‰ä»€ä¹ˆåŠŸèƒ½ï¼Ÿ",
            ]
            question = random.choice(questions)
            
            # æ„å»ºé«˜è´¨é‡ç­”æ¡ˆï¼ˆåŸºäºçœŸå®ä¿¡æ¯ï¼‰
            answer_parts = []
            
            # 1. åŸºæœ¬ä¿¡æ¯
            answer_parts.append(f"`{name}` æ˜¯ {self.project_name} é¡¹ç›®ä¸­çš„ä¸€ä¸ª {self._type_to_cn(element_type)}ï¼Œä½äº `{filepath}`ã€‚")
            
            # 2. åŠŸèƒ½æè¿°ï¼ˆæ¥è‡ªdocstringï¼‰
            if docstring:
                # æ¸…ç†docstring
                clean_doc = self._clean_docstring(docstring)
                answer_parts.append(f"\n**åŠŸèƒ½æè¿°**ï¼š\n{clean_doc}")
            
            # 3. å‡½æ•°ç­¾åï¼ˆå¦‚æœæœ‰ï¼‰
            if signature:
                answer_parts.append(f"\n**å‡½æ•°ç­¾å**ï¼š\n```python\n{signature}\n```")
            
            # 4. å‚æ•°è¯´æ˜ï¼ˆå¦‚æœæœ‰ï¼‰
            params = element.get('parameters', [])
            if params and len(params) > 0:
                param_desc = "\n**å‚æ•°**ï¼š\n"
                for param in params[:5]:  # æœ€å¤š5ä¸ªå‚æ•°
                    param_name = param.get('name', 'unknown')
                    param_type = param.get('type', 'Any')
                    # å°è¯•ä» docstring ä¸­æå–å‚æ•°æè¿°ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ç±»å‹
                    param_desc_from_doc = self._extract_param_desc(docstring, param_name)
                    if param_desc_from_doc:
                        param_info = f"- `{param_name}` ({param_type}): {param_desc_from_doc}\n"
                    else:
                        param_info = f"- `{param_name}` ({param_type})\n"

                    param_desc += param_info
                answer_parts.append(param_desc)
            
            # 5. è¿”å›å€¼ï¼ˆå¦‚æœæœ‰ï¼‰
            return_type = element.get('return_type')
            if return_type:
                answer_parts.append(f"\n**è¿”å›å€¼**ï¼š`{return_type}`")
            
            answer = ''.join(answer_parts)
            
            self.training_samples.append(TrainingSample(
                conversations=[
                    {"role": "user", "content": question},
                    {"role": "assistant", "content": answer}
                ],
                metadata={
                    "task_type": "code_explanation",
                    "element_name": name,
                    "filepath": filepath
                }
            ))
    
    def _generate_api_usage_samples(self):
        """ç”ŸæˆAPIä½¿ç”¨ç¤ºä¾‹ - åŸºäºå‡½æ•°ç­¾å"""
        # é€‰æ‹©å…¬å…±å‡½æ•°/æ–¹æ³•
        candidates = [e for e in self.code_elements 
                     if e['type'] in ['function', 'method']
                     and not e['name'].startswith('_')  # æ’é™¤ç§æœ‰æ–¹æ³•
                     and e.get('parameters')]
        
        for element in candidates[:150]: # å¢åŠ æ•°é‡é™åˆ¶
            name = element['name']
            params = element.get('parameters', [])
            filepath = element['filepath']
            docstring = element.get('docstring', '')
            
            question = f"å¦‚ä½•åœ¨ {self.project_name} ä¸­ä½¿ç”¨ `{name}` å‡½æ•°ï¼Ÿ"
            
            # æ„å»ºä½¿ç”¨ç¤ºä¾‹
            answer_parts = []
            answer_parts.append(f"`{name}` ä½äº `{filepath}`ï¼Œä½¿ç”¨æ–¹æ³•å¦‚ä¸‹ï¼š")
            
            # ç”Ÿæˆç¤ºä¾‹ä»£ç 
            param_names = [p['name'] for p in params if p['name'] != 'self']
            if param_names:
                example_code = f"{name}("
                param_examples = []
                for p in param_names[:5]:  # æœ€å¤š5ä¸ªå‚æ•°
                    param_examples.append(f"{p}=...")
                example_code += ", ".join(param_examples)
                example_code += ")"
                
                answer_parts.append(f"\n```python\n{example_code}\n```")
            
            # å‚æ•°è¯´æ˜
            if params:
                answer_parts.append("\n**å‚æ•°è¯´æ˜**ï¼š")
                for param in params[:5]:
                    if param['name'] != 'self':
                        param_type = param.get('type', 'Any')
                        
                        param_desc_from_doc = self._extract_param_desc(docstring, param['name'])
                        
                        answer_parts.append(f"\n- `{param['name']}`: {param_type}")
                        if param_desc_from_doc:
                             answer_parts[-1] += f" - {param_desc_from_doc}" # è¿½åŠ æè¿°
            
            # æ·»åŠ docstringæç¤º
            if docstring:
                clean_doc = self._clean_docstring(docstring)[:200]
                if clean_doc:
                    answer_parts.append(f"\n\n**åŠŸèƒ½ç®€è¿°**ï¼š{clean_doc}...")
            
            answer = ''.join(answer_parts)
            
            self.training_samples.append(TrainingSample(
                conversations=[
                    {"role": "user", "content": question},
                    {"role": "assistant", "content": answer}
                ],
                metadata={
                    "task_type": "api_usage",
                    "element_name": name
                }
            ))
    
    def _generate_project_overview_samples(self):
        """ç”Ÿæˆé¡¹ç›®æ¦‚è§ˆé—®ç­” - åŸºäºç»Ÿè®¡ä¿¡æ¯"""
        stats = self.analysis_data.get('statistics', {})
        description = self.project_context.get('description', '')
        techs = self.project_context.get('main_technologies', [])
        file_type_counts = self.analysis_data.get('statistics', {}).get('file_type_counts', {})
        
        # --- é—®é¢˜1: é¡¹ç›®ä¸»è¦åŠŸèƒ½ (æ›´å…·é¡¹ç›®ç‰¹è‰²) ---
        q1_list = [
            f"è¯·ç”¨ä¸€å¥è¯æè¿° {self.project_name} é¡¹ç›®çš„ä¸»è¦åŠŸèƒ½ã€‚",
            f"{self.project_name} æ˜¯ä¸€ä¸ªä»€ä¹ˆæ ·çš„é¡¹ç›®ï¼Ÿ",
            f"ç®€å•ä»‹ç»ä¸€ä¸‹ {self.project_name} é¡¹ç›®ã€‚"
        ]
        q1 = random.choice(q1_list)
        
        a1_parts = [
            f"{self.project_name} æ˜¯ä¸€ä¸ª Python é¡¹ç›®ã€‚"
        ]
        
        if description:
            # ä¿®å¤ï¼šç¡®ä¿é¡¹ç›®æè¿°æ¸…æ™°
            a1_parts.append(f"\n**æ ¸å¿ƒç›®æ ‡**ï¼š\n{description}")
        else:
             a1_parts.append("\n**æ ¸å¿ƒç›®æ ‡**ï¼šæ­¤é¡¹ç›®æ—¨åœ¨æä¾›ä¸€ä¸ªå¯æ‰©å±•çš„å¤šä»£ç†ç³»ç»Ÿæ¡†æ¶ï¼ˆAgent Frameworkï¼‰ï¼Œæ”¯æŒä»»åŠ¡è§„åˆ’ã€å·¥å…·è°ƒç”¨ã€æ¶ˆæ¯é˜Ÿåˆ—å’Œæ•°æ®åº“é›†æˆç­‰åŠŸèƒ½ã€‚")
        
        # æ·»åŠ æŠ€æœ¯æ ˆ
        if techs:
            a1_parts.append(f"\n\n**ä¸»è¦æŠ€æœ¯æ ˆ**ï¼š{', '.join(techs[:5])}ç­‰ã€‚")
        
        a1_parts.append(f"\n\né¡¹ç›®åŒ…å« {stats.get('total_elements', 0)} ä¸ªä»£ç å…ƒç´ ï¼Œä¸»è¦ç”± {stats.get('classes', 0)} ä¸ªç±»å’Œ {stats.get('functions', 0) + stats.get('methods', 0)} ä¸ªå‡½æ•°/æ–¹æ³•æ„æˆã€‚")
        
        a1 = ''.join(a1_parts)
        
        self.training_samples.append(TrainingSample(
            conversations=[
                {"role": "user", "content": q1},
                {"role": "assistant", "content": a1}
            ],
            metadata={"task_type": "project_overview"}
        ))
        
        # --- é—®é¢˜2: é¡¹ç›®ç»“æ„ ---
        q2_list = [
            f"{self.project_name} çš„é¡¹ç›®ç»“æ„æ˜¯æ€æ ·çš„ï¼Ÿ",
            f"è¯·åˆ—ä¸¾ {self.project_name} çš„æ ¸å¿ƒæ¨¡å—ã€‚",
        ]
        q2 = random.choice(q2_list)
        
        a2_parts = [f"{self.project_name} é¡¹ç›®åŒ…å«ä»¥ä¸‹ä¸»è¦éƒ¨åˆ†ï¼š\n"]
        
        # è·å–ä¸»è¦æ¨¡å—
        modules = self.project_context.get('key_modules', [])
        if modules:
            a2_parts.append("\n**æ ¸å¿ƒæ¨¡å—**ï¼š\n")
            for mod in modules[:10]:
                a2_parts.append(f"- `{mod}`\n")
        else:
            a2_parts.append("\n**æ ¸å¿ƒæ¨¡å—**ï¼š\n- `core` (æ ¸å¿ƒé€»è¾‘ï¼Œå¦‚Agent Runtime, Tooling, Config)\n- `cli` (å‘½ä»¤è¡Œæ¥å£)\n- `llms` (LLMåç«¯å®ç°)\n")

        # ä¼˜åŒ–æ–‡ä»¶ç±»å‹å±•ç¤º
        if file_type_counts:
             file_stats = ', '.join(f'{k.lstrip(".").upper()}: {v}' for k, v in file_type_counts.items() if k not in ['.other'])
             a2_parts.append(f"\n**ä¸»è¦æ–‡ä»¶ç±»å‹ç»Ÿè®¡**ï¼š{file_stats}")
        
        a2 = ''.join(a2_parts)
        
        self.training_samples.append(TrainingSample(
            conversations=[
                {"role": "user", "content": q2},
                {"role": "assistant", "content": a2}
            ],
            metadata={"task_type": "project_structure"}
        ))
        
        # --- é—®é¢˜3: æ ¸å¿ƒç±»/å‡½æ•° ---
        top_elements = sorted(self.code_elements, 
                             key=lambda x: x.get('complexity', 0), 
                             reverse=True)[:10]
        
        q3 = f"{self.project_name} ä¸­æœ‰å“ªäº›æ ¸å¿ƒç±»å’Œå‡½æ•°ï¼Ÿ"
        a3_parts = [f"{self.project_name} çš„æ ¸å¿ƒç»„ä»¶åŒ…æ‹¬ï¼ˆåŸºäºå¤æ‚åº¦å’Œé‡è¦æ€§ï¼‰ï¼š\n"]
        
        for elem in top_elements:
            name = elem['name']
            filepath = elem['filepath']
            elem_type = self._type_to_cn(elem['type'])
            
            doc = elem.get('docstring', '')
            short_doc = self._clean_docstring(doc).split('\n')[0][:80].strip()
            
            line = f"\n- `{name}` ({elem_type})ï¼šä½äº `{filepath}`"
            if short_doc:
                line += f" - {short_doc}..."
            a3_parts.append(line)
        
        if len(top_elements) > 0:
             a3 = ''.join(a3_parts)
             self.training_samples.append(TrainingSample(
                conversations=[
                    {"role": "user", "content": q3},
                    {"role": "assistant", "content": a3}
                ],
                metadata={"task_type": "core_components"}
            ))
    
    def _generate_code_location_samples(self):
        """ç”Ÿæˆä»£ç å®šä½ä»»åŠ¡"""
        # é€‰æ‹©ä¸åŒæ–‡ä»¶ä¸­çš„å…ƒç´ 
        file_elements = defaultdict(list)
        for elem in self.code_elements:
            # æ’é™¤éæ ¸å¿ƒçš„__init__
            if elem['name'] == '__init__' and 'module' not in elem['type']:
                continue
            file_elements[elem['filepath']].append(elem)
        
        for filepath, elements in list(file_elements.items())[:50]:
            # éšæœºé€‰æ‹©1-3ä¸ªå…ƒç´ 
            selected = random.sample(elements, min(3, len(elements)))
            
            for elem in selected:
                name = elem['name']
                elem_type = self._type_to_cn(elem['type'])
                
                question = f"åœ¨ {self.project_name} ä¸­ï¼Œ`{name}` {elem_type}åœ¨å“ªä¸ªæ–‡ä»¶é‡Œï¼Ÿ"
                
                # ç­”æ¡ˆä¼˜åŒ–ï¼šæ›´ç®€æ´ï¼Œå‡å°‘å†—ä½™ä¿¡æ¯ï¼Œæ¨¡å‹åªéœ€å­¦ä¹ è·¯å¾„
                answer = f"`{name}` ä½äº `{filepath}`ã€‚"
                
                self.training_samples.append(TrainingSample(
                    conversations=[
                        {"role": "user", "content": question},
                        {"role": "assistant", "content": answer}
                    ],
                    metadata={
                        "task_type": "code_location",
                        "element_name": name,
                        "filepath": filepath
                    }
                ))
    
    def _extract_signature(self, code: str, element_type: str) -> str:
        """æå–å‡½æ•°/ç±»ç­¾å"""
        if not code:
            return ""
        
        lines = code.strip().split('\n')
        signature_lines = []
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            signature_lines.append(line)
            
            # æå–å‡½æ•°/æ–¹æ³•å®šä¹‰è¡Œ
            if element_type in ['function', 'method'] and (line.startswith('def ') or line.startswith('async def ')):
                # å…¼å®¹å¤šè¡Œå‡½æ•°ç­¾å
                if not line.endswith(':'):
                    continue
                return '\n'.join(signature_lines)
            
            # æå–ç±»å®šä¹‰è¡Œ
            if element_type == 'class' and line.startswith('class '):
                 if not line.endswith(':'):
                    continue
                 return '\n'.join(signature_lines)

            # é¿å…åŒ…å«å‡½æ•°/æ–¹æ³•ä½“
            if line.endswith((':')) and not line.startswith(('def ', 'class ')):
                break

        # ä»…è¿”å›å‰å‡ è¡Œï¼Œç¡®ä¿åªåŒ…å«å®šä¹‰
        return '\n'.join(signature_lines[:5])
    
    def _clean_docstring(self, docstring: str) -> str:
        """æ¸…ç†docstring"""
        if not docstring:
            return ""
        
        # ç§»é™¤å¤šä½™ç©ºç™½
        lines = docstring.strip().split('\n')
        cleaned = []
        for line in lines:
            line = line.strip()
            if line:
                cleaned.append(line)
        
        return ' '.join(cleaned)

    def _extract_param_desc(self, docstring: str, param_name: str) -> str:
        """ä» docstring ä¸­å°è¯•æå–å‚æ•°æè¿°"""
        if not docstring:
            return ""
        # åŒ¹é…å„ç§æ ¼å¼çš„å‚æ•°æè¿°ï¼Œä¾‹å¦‚ Args: key: The cache key.
        match = re.search(rf"(?:Args|Parameters|Params):\s*(?:[\n\r]\s*-)?\s*`?{re.escape(param_name)}`?\s*[:\-]\s*(.*)", docstring, re.IGNORECASE)
        if match:
             desc = match.group(1).split('\n')[0].strip()
             return desc if desc else "æ— æè¿°"
        return ""
    
    def _type_to_cn(self, element_type: str) -> str:
        """å…ƒç´ ç±»å‹è½¬ä¸­æ–‡"""
        mapping = {
            'function': 'å‡½æ•°',
            'method': 'æ–¹æ³•',
            'class': 'ç±»',
            'variable': 'å˜é‡',
            'module': 'æ¨¡å—'
        }
        return mapping.get(element_type, element_type)
    
    def save_training_data(self):
        """ä¿å­˜è®­ç»ƒæ•°æ®"""
        output_dir = Path(self.config['dataset']['output_dir'])
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # æ‰“ä¹±
        random.shuffle(self.training_samples)
        
        # åˆ†å‰²
        total = len(self.training_samples)
        train_size = int(total * 0.8)
        val_size = int(total * 0.1)
        
        if total < 10: # å¦‚æœæ ·æœ¬å¤ªå°‘ï¼Œå¹³å‡åˆ†é…
             train_size = max(1, total // 2)
             val_size = max(1, (total - train_size) // 2)
        
        # å†æ¬¡æ£€æŸ¥ï¼Œç¡®ä¿åˆ†å‰²ä¸ä¼šå¯¼è‡´ç´¢å¼•é”™è¯¯
        if train_size + val_size > total:
             val_size = total - train_size

        train_data = self.training_samples[:train_size]
        val_data = self.training_samples[train_size:train_size + val_size]
        test_data = self.training_samples[train_size + val_size:]
        
        # ä¿å­˜ä¸ºJSONL
        self._save_jsonl(train_data, output_dir / "train.jsonl")
        self._save_jsonl(val_data, output_dir / "val.jsonl")
        self._save_jsonl(test_data, output_dir / "test.jsonl")
        
        # å…ƒæ•°æ®
        metadata = {
            'total_samples': total,
            'train_samples': len(train_data),
            'val_samples': len(val_data),
            'test_samples': len(test_data),
            'project_name': self.project_name,
            'task_distribution': self._get_task_distribution()
        }
        
        with open(output_dir / "metadata.json", 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
        
        print(f"\nâœ“ Training data saved:")
        print(f"  Train: {len(train_data)}")
        print(f"  Val: {len(val_data)}")
        print(f"  Test: {len(test_data)}")
        print(f"  Total: {total}")
        
        # æ˜¾ç¤ºæ ·æœ¬ç¤ºä¾‹
        print(f"\nğŸ“ Sample training example:")
        if train_data:
            sample = random.choice(train_data)
            print(f"Q: {sample.conversations[0]['content'][:100]}...")
            print(f"A: {sample.conversations[1]['content'][:150]}...")
    
    def _save_jsonl(self, data: List[TrainingSample], filepath: Path):
        """ä¿å­˜ä¸ºJSONLæ ¼å¼"""
        with open(filepath, 'w', encoding='utf-8') as f:
            for sample in data:
                # ä»…ä¿å­˜å¯¹è¯ï¼Œä¸ä¿å­˜ metadata
                json.dump({'conversations': sample.conversations}, f, ensure_ascii=False)
                f.write('\n')
    
    def _get_task_distribution(self) -> Dict[str, int]:
        """ç»Ÿè®¡ä»»åŠ¡åˆ†å¸ƒ"""
        dist = {}
        for sample in self.training_samples:
            task_type = sample.metadata.get('task_type', 'unknown')
            dist[task_type] = dist.get(task_type, 0) + 1
        return dist


def main():
    print("="*60)
    print("Fixed Training Data Generator (Project-Specific Answers Enhanced)")
    print("="*60)
    
    generator = FixedDataGenerator()
    generator.generate_training_data()
    generator.save_training_data()
    
    print("\n" + "="*60)
    print("âœ“ Data generation completed!")
    print("="*60)


if __name__ == "__main__":
    main()
