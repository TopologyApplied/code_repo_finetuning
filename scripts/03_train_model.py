"""
ä¿®å¤ç‰ˆæ¨¡å‹å¾®è°ƒè„šæœ¬
æ ¸å¿ƒæ”¹è¿›:
1. é²æ£’çš„æ ‡ç­¾æ©ç ï¼ˆåªå­¦ä¹ assistantçš„å›ç­”ï¼‰- æœ€ç»ˆã€æœ€é²æ£’ä¿®æ­£ç‰ˆ
2. è§£å†³ QwenTokenizer æ²¡æœ‰ im_end_id å±æ€§çš„å…¼å®¹æ€§é—®é¢˜ã€‚
3. ä¿®å¤ TypeError: '<=' not supported between instances of 'float' and 'str' é—®é¢˜ã€‚
"""
import os
import json
import yaml
import torch
from pathlib import Path
from dataclasses import dataclass, field
from typing import Optional, List
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForSeq2Seq,
    TrainerCallback,
)
from peft import LoraConfig, get_peft_model, TaskType
from datasets import load_dataset
import numpy as np


@dataclass
class ModelArguments:
    """æ¨¡å‹å‚æ•°"""
    model_name_or_path: str = field(default="Qwen/Qwen3-8B")
    use_lora: bool = field(default=True)
    lora_r: int = field(default=64)  
    lora_alpha: int = field(default=128)  
    lora_dropout: float = field(default=0.05)
    lora_target_modules: List[str] = field(
        default_factory=lambda: [
            "q_proj", "k_proj", "v_proj", "o_proj", 
            "gate_proj", "up_proj", "down_proj"
        ]
    )


@dataclass
class DataArguments:
    """æ•°æ®å‚æ•°"""
    data_dir: str = field(default="./data/training_data")
    max_length: int = field(default=1024)
    preprocessing_num_workers: int = field(default=32)


class SampleInspectionCallback(TrainerCallback):
    """è®­ç»ƒæ ·æœ¬æ£€æŸ¥å›è°ƒ"""
    def __init__(self, tokenizer):
        self.tokenizer = tokenizer
        self.checked = False
    
    def on_step_begin(self, args, state, control, **kwargs):
        """åœ¨ç¬¬ä¸€æ­¥å¼€å§‹æ—¶æ£€æŸ¥æ ·æœ¬"""
        if not self.checked and state.global_step == 0:
            print("\n" + "="*60)
            print("ğŸ” Inspecting training samples...")
            print("="*60)
            self.checked = True


class QwenFineTunerFixed:
    """Qwenæ¨¡å‹å¾®è°ƒå™¨ - ä¿®å¤ç‰ˆ"""
    config_path = Path(__file__).parent.parent / "config" / "default_config.yaml"

    def __init__(self, config_path: str = config_path):
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
        
        self.model_args = ModelArguments(
            model_name_or_path=self.config['model']['base_model']
        )
        self.data_args = DataArguments(
            data_dir=self.config['dataset']['output_dir']
        )
        
        self.output_dir = Path(self.config['training']['output_dir'])
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.tokenizer = None
        self.model = None
        self.train_dataset = None
        self.eval_dataset = None
        # æ–°å¢å±æ€§ï¼šç”¨äºå®‰å…¨å­˜å‚¨ im_end_id
        self.im_end_token_id = None 
        
    def load_tokenizer_and_model(self):
        """åŠ è½½tokenizerå’Œæ¨¡å‹"""
        print(f"Loading tokenizer from {self.model_args.model_name_or_path}")
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_args.model_name_or_path,
            trust_remote_code=True,
            padding_side='right'
        )
        
        # å®‰å…¨è·å– im_end_id (ä¿®å¤ im_end_id å±æ€§é”™è¯¯)
        try:
             # Qwen token ID æ˜¯ 151644
             self.im_end_token_id = self.tokenizer.convert_tokens_to_ids("<|im_end|>")
             if self.im_end_token_id is None:
                 raise ValueError("Could not convert <|im_end|> token to ID.")
        except Exception as e:
             print(f"Warning: Could not get <|im_end|> ID, trying fallback: {e}")
             self.im_end_token_id = self.tokenizer.eos_token_id
        print(f"Using im_end_id: {self.im_end_token_id}")

        
        # è®¾ç½®pad_token
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id
            
        if self.tokenizer.chat_template is None:
             print("Warning: Qwen chat template not found. Using default template logic.")
        
        print(f"Loading model from {self.model_args.model_name_or_path}")
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_args.model_name_or_path,
            torch_dtype=torch.bfloat16,
            trust_remote_code=True,
            use_cache=False,
            low_cpu_mem_usage=True
        )
        
        # å‡†å¤‡LoRA
        print("Preparing model for LoRA training...")
        if self.model_args.use_lora:
             
            print("Applying LoRA configuration")
            lora_config = LoraConfig(
                task_type=TaskType.CAUSAL_LM,
                r=self.model_args.lora_r,
                lora_alpha=self.model_args.lora_alpha,
                lora_dropout=self.model_args.lora_dropout,
                target_modules=self.model_args.lora_target_modules,
                bias="none",
                inference_mode=False,
            )
            
            self.model = get_peft_model(self.model, lora_config)
            self.model.print_trainable_parameters()
            self.model.train()
            
            # éªŒè¯
            trainable = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
            print(f"âœ“ Trainable parameters: {trainable:,}")
    
    def load_and_preprocess_data(self):
        """åŠ è½½å’Œé¢„å¤„ç†æ•°æ®"""
        print("Loading datasets...")
        
        data_files = {
            'train': str(Path(self.data_args.data_dir) / 'train.jsonl'),
            'validation': str(Path(self.data_args.data_dir) / 'val.jsonl'),
        }
        
        raw_datasets = load_dataset('json', data_files=data_files)
        
        print("Preprocessing datasets...")
        self.train_dataset = raw_datasets['train'].map(
            self._preprocess_function,
            batched=True,
            num_proc=self.data_args.preprocessing_num_workers,
            remove_columns=raw_datasets['train'].column_names,
            desc="Preprocessing train dataset"
        )
        
        self.eval_dataset = raw_datasets['validation'].map(
            self._preprocess_function,
            batched=True,
            num_proc=self.data_args.preprocessing_num_workers,
            remove_columns=raw_datasets['validation'].column_names,
            desc="Preprocessing validation dataset"
        )
        
        # è¿‡æ»¤è¿‡é•¿æ ·æœ¬
        print("Filtering samples...")
        self.train_dataset = self.train_dataset.filter(
            lambda x: x is not None and len(x['input_ids']) <= self.data_args.max_length
        )
        self.eval_dataset = self.eval_dataset.filter(
            lambda x: x is not None and len(x['input_ids']) <= self.data_args.max_length
        )
        
        print(f"âœ“ Train samples: {len(self.train_dataset)}")
        print(f"âœ“ Validation samples: {len(self.eval_dataset)}")
        
        # æ£€æŸ¥ç¬¬ä¸€ä¸ªæ ·æœ¬
        if len(self.train_dataset) > 0:
            self._inspect_sample(self.train_dataset[0])
    
    def _preprocess_function(self, examples):
        """é¢„å¤„ç†å‡½æ•° - æœ€ç»ˆã€æœ€é²æ£’ä¿®æ­£ç‰ˆæ ‡ç­¾æ©ç """
        model_inputs = {
            "input_ids": [],
            "attention_mask": [],
            "labels": []
        }
        
        for conversations in examples['conversations']:
            try:
                # 1. å®Œæ•´å¯¹è¯æ–‡æœ¬
                full_text = self.tokenizer.apply_chat_template(
                    conversations,
                    tokenize=False,
                    add_generation_prompt=False 
                )
                
                # æ‰¾åˆ°æœ€åä¸€ä¸ª Assistant æ¶ˆæ¯çš„ç´¢å¼•
                last_assistant_index = next((i for i, msg in reversed(list(enumerate(conversations))) if msg['role'] == 'assistant'), -1)
                
                if last_assistant_index == -1:
                    print("Warning: Skipping conversation with no assistant reply.")
                    continue
                
                # æ„é€  "ä»…é—®é¢˜" çš„å¯¹è¯åˆ—è¡¨: åŒ…å«æ‰€æœ‰æ¶ˆæ¯ç›´åˆ°æœ€åä¸€ä¸ª Assistant æ¶ˆæ¯ä¹‹å‰
                prompt_messages = conversations[:last_assistant_index]
                # åŠ ä¸Šæœ€åä¸€ä¸ª Assistant æ¶ˆæ¯çš„ Role Prompt (ä¾‹å¦‚ <|im_start|>assistant\n)
                prompt_messages.append({"role": "assistant", "content": ""}) 
                
                prompt_text = self.tokenizer.apply_chat_template(
                    prompt_messages,
                    tokenize=False,
                    add_generation_prompt=False 
                )
                
                # 3. åˆ†è¯: å®Œæ•´å¯¹è¯
                tokenized_full = self.tokenizer(
                    full_text,
                    max_length=self.data_args.max_length,
                    truncation=True,
                    padding=False,
                )
                
                # 4. åˆ†è¯: ä»…é—®é¢˜éƒ¨åˆ† (è·å–ç­”æ¡ˆèµ·å§‹ç‚¹)
                tokenized_prompt = self.tokenizer(
                    prompt_text,
                    max_length=self.data_args.max_length,
                    truncation=True,
                    padding=False,
                )
                
                input_ids = tokenized_full['input_ids']
                labels = input_ids.copy()
                
                # ç­”æ¡ˆå†…å®¹çš„èµ·å§‹ç´¢å¼• = ä»…é—®é¢˜éƒ¨åˆ†çš„é•¿åº¦
                answer_start_index = len(tokenized_prompt['input_ids'])
                
                if answer_start_index >= len(labels):
                    print(f"Warning: Answer start index {answer_start_index} exceeds or matches total length {len(labels)}. Skipping.")
                    return None

                # 5. æ ‡ç­¾æ©ç :
                # æ©ç›–æ‰ç­”æ¡ˆèµ·å§‹ç‚¹ä¹‹å‰çš„æ‰€æœ‰ tokens
                labels[:answer_start_index] = [-100] * answer_start_index
                
                # ç¡®ä¿æœ€åä¸€ä¸ª token (é€šå¸¸æ˜¯ EOS/PAD æˆ– <|im_end|>) ä¹Ÿè¢«æ©ç›–
                if len(labels) > 0:
                    last_token_id = labels[-1]
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ EOS/PAD token
                    if last_token_id != -100 and last_token_id == self.tokenizer.eos_token_id:
                        labels[-1] = -100
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ Qwen çš„ <|im_end|> token (ä½¿ç”¨å®‰å…¨å­˜å‚¨çš„ ID)
                    if self.im_end_token_id is not None and last_token_id != -100 and last_token_id == self.im_end_token_id:
                        labels[-1] = -100
                
                model_inputs["input_ids"].append(input_ids)
                model_inputs["attention_mask"].append(tokenized_full['attention_mask'])
                model_inputs["labels"].append(labels)
            
            except Exception as e:
                import sys
                import traceback
                traceback.print_exc(file=sys.stdout)
                print(f"Error processing conversation: {e}")
                return None
                
        return model_inputs
    
    # ... (_inspect_sample æ–¹æ³•ä¿æŒä¸å˜)
    def _inspect_sample(self, sample):
        """æ£€æŸ¥æ ·æœ¬è´¨é‡"""
        print("\n" + "="*60)
        print("ğŸ” Sample Inspection (AFTER FINAL, MOST ROBUST FIXES)")
        print("="*60)
        
        input_ids = sample['input_ids']
        labels = sample['labels']
        
        # è§£ç 
        input_text = self.tokenizer.decode(input_ids, skip_special_tokens=False)
        
        # ç»Ÿè®¡
        total_tokens = len(input_ids)
        masked_tokens = sum(1 for l in labels if l == -100)
        learning_tokens = total_tokens - masked_tokens
        
        print(f"Total tokens: {total_tokens}")
        print(f"Masked tokens (prompt/padding): {masked_tokens} ({masked_tokens/total_tokens*100:.1f}%)")
        print(f"Learning tokens (assistant): {learning_tokens} ({learning_tokens/total_tokens*100:.1f}%)")
        
        # æ˜¾ç¤ºå‰200ä¸ªtokençš„æ©ç æƒ…å†µ
        print("\nğŸ“Š First 200 tokens masking pattern:")
        preview_len = min(200, len(labels))
        mask_preview = ''.join(['â–ˆ' if labels[i] == -100 else 'â–‘' for i in range(preview_len)])
        
        # æ‰¾åˆ°ç¬¬ä¸€ä¸ªå­¦ä¹  token å’Œç¬¬ä¸€ä¸ªæ©ç  token
        first_learn_idx = next((i for i, l in enumerate(labels) if l != -100), -1)
        
        if first_learn_idx != -1:
             print(f"First 10 tokens: {self.tokenizer.decode(input_ids[:10], skip_special_tokens=False)}")
             print(f"First learning token index: {first_learn_idx}")
             print(f"First learning token: {self.tokenizer.decode(input_ids[first_learn_idx])}")
             # æ‰“å°å­¦ä¹ å†…å®¹å‘¨å›´çš„ tokens
             start = max(0, first_learn_idx - 5)
             end = min(len(input_ids), first_learn_idx + 5)
             print(f"Around learning start: {self.tokenizer.decode(input_ids[start:end], skip_special_tokens=False)}")

        print(mask_preview)
        print("â–ˆ = masked (prompt/padding) | â–‘ = learning (assistant)")
        
        # æ˜¾ç¤ºå­¦ä¹ å†…å®¹ç¤ºä¾‹
        learning_ids = [input_ids[i] for i in range(len(labels)) if labels[i] != -100]
        if learning_ids:
            learning_text = self.tokenizer.decode(learning_ids[:100], skip_special_tokens=True)
            print(f"\nğŸ“ Learning content preview:")
            print(f"{learning_text[:200]}...")
        
        print("="*60 + "\n")
    
    def train(self):
        """è®­ç»ƒæ¨¡å‹"""
        print("Setting up training arguments...")
        
        # æ”¹è¿›çš„è®­ç»ƒé…ç½®
        training_args = TrainingArguments(
            output_dir=str(self.output_dir),
            num_train_epochs=self.config['training']['num_epochs'],
            
            # æ‰¹æ¬¡é…ç½®
            per_device_train_batch_size=2, 
            per_device_eval_batch_size=2,
            gradient_accumulation_steps=8, 
            
            # å­¦ä¹ ç‡
            learning_rate=float(self.config['training']['learning_rate']), # <--- ä¿®å¤: å¼ºåˆ¶ç±»å‹è½¬æ¢ float
            warmup_ratio=float(self.config['training']['warmup_ratio']), # <--- ä¿®å¤: å¼ºåˆ¶ç±»å‹è½¬æ¢ float
            lr_scheduler_type="cosine",
            
            # ä¼˜åŒ–å™¨
            optim="adamw_torch",
            weight_decay=float(self.config['training']['weight_decay']), # <--- ä¿®å¤: å¼ºåˆ¶ç±»å‹è½¬æ¢ float
            max_grad_norm=float(self.config['training']['max_grad_norm']), # <--- ä¿®å¤: å¼ºåˆ¶ç±»å‹è½¬æ¢ float
            
            # æ—¥å¿—å’Œä¿å­˜
            logging_steps=10,
            save_steps=100,
            eval_steps=100,
            save_total_limit=3,
            
            # è¯„ä¼°
            eval_strategy="steps",
            save_strategy="steps",
            load_best_model_at_end=True,
            metric_for_best_model="eval_loss",
            greater_is_better=False,
            
            # ç²¾åº¦
            bf16=True,
            bf16_full_eval=True,
            
            # DeepSpeed
            deepspeed="../config/deepspeed_zero3.json",
            
            # å…¶ä»–
            report_to=["tensorboard"],
            logging_dir=str(self.output_dir / "logs"),
            remove_unused_columns=False,
            dataloader_pin_memory=True,
            dataloader_num_workers=0,
            logging_first_step=True,
            logging_nan_inf_filter=True,
        )
        
        # Data collator
        data_collator = DataCollatorForSeq2Seq(
            tokenizer=self.tokenizer,
            model=self.model,
            label_pad_token_id=-100,
            padding=True,
        )
        
        # Callbacks
        callbacks = [SampleInspectionCallback(self.tokenizer)]
        
        # Trainer
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=self.train_dataset,
            eval_dataset=self.eval_dataset,
            data_collator=data_collator,
            tokenizer=self.tokenizer,
            callbacks=callbacks,
        )
        
        # è®­ç»ƒå‰éªŒè¯
        print("\n" + "="*60)
        print("Pre-training Validation")
        print("="*60)
        print(f"âœ“ Model in training mode: {self.model.training}")
        
        lora_params = sum(p.numel() for n, p in self.model.named_parameters() 
                         if p.requires_grad and 'lora' in n.lower())
        print(f"âœ“ LoRA parameters: {lora_params:,}")
        
        # å¼€å§‹è®­ç»ƒ
        print("\n" + "="*60)
        print("Starting Training")
        print("="*60)
        
        train_result = trainer.train()
        
        # ä¿å­˜
        print("\nSaving model...")
        trainer.save_model(str(self.output_dir / "final_model"))
        
        # ä¿å­˜æŒ‡æ ‡
        metrics = train_result.metrics
        trainer.log_metrics("train", metrics)
        trainer.save_metrics("train", metrics)
        
        # è¯„ä¼°
        print("\nEvaluating...")
        eval_metrics = trainer.evaluate()
        trainer.log_metrics("eval", eval_metrics)
        trainer.save_metrics("eval", eval_metrics)
        
        print("\nâœ“ Training completed!")
        return trainer


def main():
    """ä¸»å‡½æ•°"""
    if 'CUDA_VISIBLE_DEVICES' not in os.environ:
         os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'
    if 'TOKENIZERS_PARALLELISM' not in os.environ:
         os.environ['TOKENIZERS_PARALLELISM'] = 'false'
    if 'PYTORCH_CUDA_ALLOC_CONF' not in os.environ:
         os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
    
    print("="*60)
    print("Qwen3-8B Fine-tuning - Fixed Version (Label Masking/LoRA Params Improved)")
    print("="*60)
    print()
    
    finetuner = QwenFineTunerFixed()
    finetuner.load_tokenizer_and_model()
    finetuner.load_and_preprocess_data()
    trainer = finetuner.train()
    
    print("\n" + "="*60)
    print("âœ“ Fine-tuning Complete!")
    print(f"Model saved to: {finetuner.output_dir}")
    print("="*60)


if __name__ == "__main__":
    main()

